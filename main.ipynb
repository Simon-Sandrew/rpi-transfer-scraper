{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once you have a page open, scraping the classes on that page\n",
    "def scrape_class_data(driver, institution_name):\n",
    "    print(f\"\\n{'='*20} Scraping courses for {institution_name} {'='*20}\")\n",
    "    \n",
    "    courses = []\n",
    "  \n",
    "    # get the number of courses\n",
    "    elements = driver.find_elements(By.CSS_SELECTOR, '[id^=\"gdvCourseEQ_lblReceiveCourseCode_\"]')\n",
    "    num_courses = len(elements)\n",
    "    \n",
    "\n",
    "    # loop through each course and get the other uni name and rpi course names\n",
    "    for i in range(num_courses):\n",
    "        rpi_course_name = driver.find_element(By.ID, f\"gdvCourseEQ_lblReceiveCourseCode_{i}\")\n",
    "        alt_course_name = driver.find_element(By.ID, f\"gdvCourseEQ_btnViewCourseEQDetail_{i}\")\n",
    "\n",
    "        courses_data = {\n",
    "            \"alt_course_name\": alt_course_name.text,\n",
    "            \"rpi_course_name\": rpi_course_name.text\n",
    "        }\n",
    "        courses.append(courses_data)\n",
    "    \n",
    "    print(f\"Found {num_courses} courses on current page\")\n",
    "    return courses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaping a page itself (deals with pagination and what not)\n",
    "def scrape_uni_page(driver, institution_name):\n",
    "    print(f\"\\n{'='*20} Processing {institution_name} {'='*20}\")\n",
    "    # get the first page\n",
    "    all_courses = scrape_class_data(driver, institution_name)\n",
    "\n",
    "    try:\n",
    "        # number of pages\n",
    "        pagination_table = driver.find_element(By.CLASS_NAME, \"pagination-tes\").find_element(By.TAG_NAME, \"table\")\n",
    "        page_cells = pagination_table.find_elements(By.TAG_NAME, \"td\")\n",
    "        num_pages = len(page_cells)\n",
    "        print(f\"Found {num_pages} total pages to process\")\n",
    "\n",
    "        for page in range(2, num_pages + 1):\n",
    "            print(f\"\\n--- Processing Page {page} ---\")\n",
    "            # Need to refind the pagination table and link each time since page reloads\n",
    "            pagination_table = driver.find_element(By.CLASS_NAME, \"pagination-tes\").find_element(By.TAG_NAME, \"table\")\n",
    "            page_link = pagination_table.find_element(By.LINK_TEXT, str(page))\n",
    "            page_link.click()\n",
    "            time.sleep(2)\n",
    "            # get the courses and extend\n",
    "            curr_page_courses = scrape_class_data(driver, institution_name)\n",
    "            all_courses.extend(curr_page_courses)\n",
    "    except:\n",
    "        # No pagination table found - just continue with single page results\n",
    "        print(\"No pagination found - single page only\")\n",
    " \n",
    "    print(f\"\\nTotal courses found for {institution_name}: {len(all_courses)}\")\n",
    "    return all_courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# nice function that lets u navigate to a specific page and deals with the annoying ... pagination at the top. \n",
    "# This is for selecting the page \n",
    "def navigate_to_page(driver, page_number):\n",
    "    # Find the pagination table\n",
    "    pagination_table = driver.find_element(By.CLASS_NAME, \"pagination-tes\").find_element(By.TAG_NAME, \"table\")\n",
    "    \n",
    "    if page_number > 10:\n",
    "        # Need to click dots first to see pages 11-14\n",
    "        dots_link = pagination_table.find_element(By.LINK_TEXT, \"...\")\n",
    "        dots_link.click()\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Re-find pagination table since page updated\n",
    "        pagination_table = driver.find_element(By.CLASS_NAME, \"pagination-tes\").find_element(By.TAG_NAME, \"table\")\n",
    "        \n",
    "        # Don't need to click page number if it's 11 since dots takes us there\n",
    "        if page_number > 11:\n",
    "            page_link = pagination_table.find_element(By.LINK_TEXT, str(page_number))\n",
    "            page_link.click()\n",
    "            time.sleep(2)\n",
    "    else:\n",
    "        # For pages 2-10, just click the page number\n",
    "        page_link = pagination_table.find_element(By.LINK_TEXT, str(page_number)) \n",
    "        page_link.click()\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# driver function\n",
    "def scrape_institutions(driver, page_numbers):\n",
    "    all_institutions_data = []\n",
    "\n",
    "\n",
    "    for page_number in page_numbers:\n",
    "        # Visit the website if first page, otherwise navigate\n",
    "        if page_number == page_numbers[0]:\n",
    "            url = \"https://tes.collegesource.com/publicview/TES_publicview01.aspx?rid=f080a477-bff8-46df-a5b2-25e9affdd4ed&aid=27b576bb-cd07-4e57-84d0-37475fde70ce\"\n",
    "            driver.get(url)\n",
    "            \n",
    "            time.sleep(60) # in case captcha is triggered, you might have to manually solve yourself\n",
    "            if page_number > 1:\n",
    "                navigate_to_page(driver, page_number)\n",
    "        else:\n",
    "            navigate_to_page(driver, page_number)\n",
    "\n",
    "        # Loop through buttons 0-49\n",
    "        for i in range(50):\n",
    "            # Find and click the button with dynamic index\n",
    "            button_id = f\"gdvInstWithEQ_btnCreditFromInstName_{i}\"\n",
    "            button = driver.find_element(By.ID, button_id)\n",
    "            institution_name = button.text\n",
    "            print(f\"\\n{'#'*80}\\nProcessing institution {i+1}/50: {institution_name}\\n{'#'*80}\")\n",
    "            try:\n",
    "                button.click()\n",
    "                # Wait for data to change by checking for presence of course table\n",
    "                WebDriverWait(driver, 20).until(\n",
    "                    EC.presence_of_element_located((By.ID, \"gdvCourseEQ\"))\n",
    "                )\n",
    "            except:\n",
    "                # if we fail, notify and just refresh the page, solve the captcha, continue\n",
    "                print(f\"Failed to click institution {institution_name} with page number {page_number}\")\n",
    "                time.sleep(90)\n",
    "                continue\n",
    "                \n",
    "            # scrape the page\n",
    "            courses = scrape_uni_page(driver, institution_name)\n",
    "\n",
    "            institution_data = {\n",
    "                \"institution_name\": institution_name,\n",
    "                \"courses\": courses\n",
    "            }\n",
    "            all_institutions_data.append(institution_data)\n",
    "            # click back to the first page, click btnSwitchView\n",
    "            driver.find_element(By.ID, \"btnSwitchView\").click()\n",
    "            # Wait for the institution list to be visible again\n",
    "            try:\n",
    "                WebDriverWait(driver, 20).until(\n",
    "                    EC.presence_of_element_located((By.ID, \"gdvInstWithEQ\"))\n",
    "                )\n",
    "            except:\n",
    "                # if we fail, notify and just refresh the page, solve the captcha, continue. This happens like 0.5 times per page\n",
    "                # depending on how many you're scraping at once\n",
    "                print(f\"Failed to click back to first page after {institution_name}, with page number {page_number}\")\n",
    "                time.sleep(90)\n",
    "                continue\n",
    "\n",
    "    print(f\"\\n{'='*50}\\nProcessing Complete\\n{'='*50}\")\n",
    "    print(f\"Total institutions processed: {len(all_institutions_data)}\")\n",
    "    for inst in all_institutions_data:\n",
    "        print(f\"\\nInstitution: {inst['institution_name']}\")\n",
    "        print(f\"Number of courses: {len(inst['courses'])}\")\n",
    "        \n",
    "    return all_institutions_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "all_data = []\n",
    "lock = threading.Lock()\n",
    "\n",
    "# drivers driver function lol\n",
    "def process_pages(page_range):\n",
    "    driver = webdriver.Chrome(options=webdriver.ChromeOptions())\n",
    "    try:\n",
    "        data = scrape_institutions(driver, page_range)\n",
    "        if data:\n",
    "            with lock:\n",
    "                all_data.extend(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing pages {page_range}: {str(e)}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Define the page ranges for each thread, run a single browser per thread\n",
    "# I generally did 4 because it was fast enough to where if the captcha was triggered,\n",
    "# you could just solve it yourself. \n",
    "page_ranges = [\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12],\n",
    "    [13, 14] \n",
    "]\n",
    "\n",
    "# uncomment to run\n",
    "\n",
    "# threads = []\n",
    "# for page_range in page_ranges:\n",
    "#     thread = threading.Thread(target=process_pages, args=(page_range,))\n",
    "#     threads.append(thread)\n",
    "#     thread.start()\n",
    "#     print(f\"Started thread for pages {page_range}\")\n",
    "\n",
    "# # Wait for all threads to complete\n",
    "# for thread in threads:\n",
    "#     thread.join()\n",
    "\n",
    "# print(f\"Total institutions collected: {len(all_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun a specific page, which can be useful if u mess up, uncomment to run\n",
    "# page_range = [8, 8]\n",
    "# data = scrape_institutions(page_range)\n",
    "# all_data.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to course_data.xlsx with 3159 courses from 388 institutions\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# converting to csv and excel\n",
    "with open('all_data_clean.json', 'r') as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "    # Convert the data to a pandas DataFrame for easier spreadsheet export\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Flatten the nested data structure\n",
    "    flattened_data = []\n",
    "    for institution in all_data:\n",
    "        inst_name = institution['institution_name']\n",
    "        for course in institution['courses']:\n",
    "            course_data = {\n",
    "                'Institution': inst_name,\n",
    "                'Alt Course': course['alt_course_name'], \n",
    "                'RPI Course': course['rpi_course_name']   \n",
    "            }\n",
    "            flattened_data.append(course_data)\n",
    "    \n",
    "    df = pd.DataFrame(flattened_data)\n",
    "    \n",
    "    df.to_excel('course_data.xlsx', index=False)\n",
    "\n",
    "    df.to_csv('course_data.csv', index=False)\n",
    "\n",
    "    print(f\"Data exported to course_data.xlsx with {len(df)} courses from {len(all_data)} institutions\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
